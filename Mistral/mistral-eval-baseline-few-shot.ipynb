{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-02T10:15:19.872213Z",
     "iopub.status.busy": "2026-02-02T10:15:19.871630Z",
     "iopub.status.idle": "2026-02-02T10:16:11.515675Z",
     "shell.execute_reply": "2026-02-02T10:16:11.515094Z",
     "shell.execute_reply.started": "2026-02-02T10:15:19.872185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install trl\n",
    "!pip install -U bitsandbytes\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, EarlyStoppingCallback\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "set_seed(2026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T10:16:11.517301Z",
     "iopub.status.busy": "2026-02-02T10:16:11.517065Z",
     "iopub.status.idle": "2026-02-02T10:18:34.892376Z",
     "shell.execute_reply": "2026-02-02T10:18:34.891737Z",
     "shell.execute_reply.started": "2026-02-02T10:16:11.517278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side=\"left\"\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "def get_prompt(data, task, variety, source, few_shot):\n",
    "    prompts = []\n",
    "    if not few_shot:\n",
    "        for (text, label) in zip(data[\"text\"], data[\"label\"]):\n",
    "            if task == \"Sentiment\":\n",
    "                prompt = f\"\"\"<s>[INST] Generate the sentiment of the given text. 1 for positive sentiment, and 0 for negative sentiment. Do not give an explanation.\\nText:{text}[/INST]\"\"\"\n",
    "            elif task == \"Sarcasm\":\n",
    "                prompt = f\"\"\"<s>[INST] Predict if the given text is sarcastic. 1 if the text is sarcastic, and 0 if the text is not sarcastic. Do not give an explanation.\\nText:{text}[/INST]\"\"\"\n",
    "            prompts.append(prompt)\n",
    "    else:\n",
    "        few_shot_examples = {\n",
    "            (\"Sentiment\", \"en-IN\", \"Google\"): [\n",
    "                (\"The hospitality was top-notch and the location is perfect for families. The authentic South Indian filter coffee is a must-try!\", 1),\n",
    "                (\"Very poor hygiene standards. The tables were sticky and the staff was extremely dismissive when we complained about the cold food.\", 0)\n",
    "            ],\n",
    "            (\"Sentiment\", \"en-IN\", \"Reddit\"): [\n",
    "                (\"Finally got my Zepto delivery in under 10 minutes. This level of convenience is such a lifesaver in Mumbai traffic!\", 1),\n",
    "                (\"Seriously fed up with these 'aesthetic' cafes in Indiranagar charging 500 bucks for mediocre cold coffee and bad vibes.\", 0)\n",
    "            ],\n",
    "            (\"Sentiment\", \"en-AU\", \"Google\"): [\n",
    "                (\"Found this absolute gem in Brunswick. The smashed avo was perfectly seasoned and the staff were total legends. Will be back!\", 1),\n",
    "                (\"Wait time was ridiculous—over 50 minutes for two burgers that came out lukewarm. Way overpriced for the quality of service.\", 0)\n",
    "            ],\n",
    "            (\"Sentiment\", \"en-AU\", \"Reddit\"): [\n",
    "                (\"The community support in r/melbourne during the power outages was actually quite heartening to see.\", 1),\n",
    "                (\"Centrelink's online portal is an absolute joke. Been trying to upload one document for three hours and it keeps crashing.\", 0)\n",
    "            ],\n",
    "            (\"Sentiment\", \"en-UK\", \"Google\"): [\n",
    "                (\"Proper Sunday roast with massive Yorkshires and plenty of gravy. The staff made us feel right at home. Brilliant value for money.\", 1),\n",
    "                (\"The hotel was a bit of a shambles. The room smelled of damp and the 'continental breakfast' was just a box of dry cereal.\", 0)\n",
    "            ],\n",
    "            (\"Sentiment\", \"en-UK\", \"Reddit\"): [\n",
    "                (\"It's great to see more investment going into local high streets; the new pedestrian zone in the city centre looks lovely.\", 1),\n",
    "                (\"The current state of the NHS wait times is terrifying. It shouldn't take six months just to see a specialist.\", 0)\n",
    "            ],\n",
    "            (\"Sarcasm\", \"en-IN\", \"Reddit\"): [\n",
    "                (\"Oh brilliant, another 2-hour power cut right in the middle of a heatwave. Truly living the 'Digital India' dream.\", 1),\n",
    "                (\"The new terminal at the Bengaluru airport is actually quite efficient and the greenery makes it very pleasant.\", 0)\n",
    "            ],\n",
    "            (\"Sarcasm\", \"en-AU\", \"Reddit\"): [\n",
    "                (\"Fantastic, another interest rate hike. I was just thinking my mortgage wasn't quite high enough yet.\", 1),\n",
    "                (\"I think the government needs to prioritize long-term infrastructure over short-term political gains.\", 0)\n",
    "            ],\n",
    "            (\"Sarcasm\", \"en-UK\", \"Reddit\"): [\n",
    "                (\"Lovely weather we're having—I especially enjoy the horizontal rain and the smell of raw sewage in the Thames. Peak Britain.\", 1),\n",
    "                (\"The volunteer-led library in our village has been doing a wonderful job providing resources for the kids.\", 0)\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for (text, label) in zip(data[\"text\"], data[\"label\"]):\n",
    "            examples = few_shot_examples.get((task, variety, source), [])\n",
    "            shot_text = \"\"\n",
    "            for ex_text, ex_label in examples:\n",
    "                shot_text += f\"Text: {ex_text}\\n{ex_label}\\n\\n\"\n",
    "            \n",
    "            if task == \"Sentiment\":\n",
    "                prompt = f\"\"\"<s>[INST] Generate the sentiment of the given text. 1 for positive sentiment, and 0 for negative sentiment. Do not give an explanation.{shot_text}Text:{text}[/INST]\"\"\"\n",
    "            elif task == \"Sarcasm\":\n",
    "                prompt = f\"\"\"<s>[INST] Predict if the following text is sarcastic. 1 if the text is sarcastic, and 0 if the text is not sarcastic. Do not give an explanation.{shot_text}Text:{text}[/INST]\"\"\"\n",
    "            prompts.append(prompt)\n",
    "            \n",
    "    return prompts\n",
    "\n",
    "def parse_prediction(prediction: str) -> int:\n",
    "    if prediction is None:\n",
    "        return -1\n",
    "\n",
    "    prediction = str(prediction)\n",
    "\n",
    "    match = re.search(r'\\b[01]\\b', prediction)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "\n",
    "    return -1\n",
    "\n",
    "def run_inference(model, df_test, task, source, variety, finetuning, few_shot):\n",
    "    \n",
    "    prompts = get_prompt(df_test, \n",
    "                         task, \n",
    "                         variety, \n",
    "                         source, \n",
    "                         few_shot)\n",
    "    labels = df_test[\"label\"].to_numpy()\n",
    "    preds = []\n",
    "    preds_str = []\n",
    "\n",
    "    for prompt in tqdm(prompts, desc=f\"Evaluation (task: {task}, source: {source}, variety: {variety}, ft: {finetuning}, few_shot: {few_shot}\"):\n",
    "        encodings = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=encodings[\"input_ids\"],\n",
    "                attention_mask=encodings[\"attention_mask\"],\n",
    "                max_length=encodings[\"input_ids\"].shape[1] + 6,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        decoded = tokenizer.batch_decode(generated_ids)\n",
    "        pred_str = decoded[0].split(\"[/INST]\", 1)[1].strip()\n",
    "        preds_str.append(pred_str)\n",
    "        preds.append(parse_prediction(pred_str))\n",
    "        \n",
    "        torch.cuda.empty_cache()     \n",
    "        del generated_ids\n",
    "        del decoded\n",
    "        del encodings\n",
    "\n",
    "    \n",
    "    preds = np.array(preds)\n",
    "\n",
    "    mask = preds != -1 # Consider only valid predictions\n",
    "    labels = labels[mask]\n",
    "    preds = preds[mask]\n",
    "\n",
    "    labels = (labels == 1).astype(int)\n",
    "    preds = (preds == 1).astype(int)\n",
    "\n",
    "    print(\"Predictions (str):\", preds_str)\n",
    "    print(\"Predictions:\",preds)\n",
    "    print(\"Labels:\",labels)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", pos_label=1)\n",
    "    metrics = {\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "    print(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T10:18:34.893624Z",
     "iopub.status.busy": "2026-02-02T10:18:34.893332Z",
     "iopub.status.idle": "2026-02-02T10:18:34.953255Z",
     "shell.execute_reply": "2026-02-02T10:18:34.952661Z",
     "shell.execute_reply.started": "2026-02-02T10:18:34.893579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Read test data\n",
    "\n",
    "df_test = pd.read_csv(\"/kaggle/input/besstie/valid.csv\")\n",
    "df_test = df_test.dropna(subset=['text', 'label', 'variety', 'source', 'task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T10:18:34.954920Z",
     "iopub.status.busy": "2026-02-02T10:18:34.954654Z",
     "iopub.status.idle": "2026-02-02T14:34:29.035641Z",
     "shell.execute_reply": "2026-02-02T14:34:29.034848Z",
     "shell.execute_reply.started": "2026-02-02T10:18:34.954881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compute performance\n",
    "\n",
    "finetuned_models_path = \"/kaggle/input/finetuned-models/finetuned_models\"\n",
    "output_path = \"/kaggle/working\"\n",
    "\n",
    "results = []\n",
    "\n",
    "df_grouped_test = df_test.groupby(['variety', 'source', 'task'])\n",
    "\n",
    "for (variety, source, task), test_section in df_grouped_test:\n",
    "  \n",
    "    df_test_subset = df_test[(df_test['variety'] == variety) & (df_test['source'] == source) & (df_test['task'] == task)]\n",
    "\n",
    "    model_name = f\"{variety}_{source}_{task}\".replace(\" \", \"_\")\n",
    "    finetuned_model_path = os.path.join(finetuned_models_path, model_name)\n",
    "    if not os.path.isdir(finetuned_model_path):\n",
    "        print(f\"Model {model_name} not found (path: {finetuned_model_path})\")\n",
    "        continue\n",
    "\n",
    "    for finetuning in [False, True]:\n",
    "        if finetuning:\n",
    "            model = PeftModel.from_pretrained(base_model, finetuned_model_path, use_safetensors=True)\n",
    "            \n",
    "            for few_shot in [False, True]:\n",
    "                metrics = run_inference(model, df_test_subset, task, source, variety, finetuning, few_shot)\n",
    "\n",
    "                metrics.update({\n",
    "                  \"finetuning\": finetuning, \"few_shot\": few_shot, \"source\": source, \"task\": task, \"variety\": variety\n",
    "                })\n",
    "                results.append(metrics)\n",
    "\n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                low_cpu_mem_usage=True,\n",
    "                return_dict=True,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=bnb_config\n",
    "            )\n",
    "                            \n",
    "        else:\n",
    "            for few_shot in [False, True]:\n",
    "                metrics = run_inference(base_model, df_test_subset, task, source, variety, finetuning, few_shot)\n",
    "            \n",
    "                metrics.update({\n",
    "                  \"finetuning\": finetuning, \"few_shot\": few_shot, \"source\": source, \"task\": task, \"variety\": variety\n",
    "                })\n",
    "                results.append(metrics)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(os.path.join(output_path, f\"mistral_results_baseline_and_few_shot.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T14:34:29.037020Z",
     "iopub.status.busy": "2026-02-02T14:34:29.036716Z",
     "iopub.status.idle": "2026-02-02T14:34:29.043927Z",
     "shell.execute_reply": "2026-02-02T14:34:29.042852Z",
     "shell.execute_reply.started": "2026-02-02T14:34:29.036996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check LoRA \n",
    "'''\n",
    "finetuned_models_path = \"/kaggle/input/finetuned-models/finetuned_models\"\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "  cls = bnb.nn.Linear4bit\n",
    "  lora_module_names = set()\n",
    "  for name, module in model.named_modules():\n",
    "    if isinstance(module, cls):\n",
    "      names = name.split('.')\n",
    "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "      lora_module_names.remove('lm_head')\n",
    "  return list(lora_module_names)\n",
    "\n",
    "target_modules = find_all_linear_names(base_model)\n",
    "print(\"1) Base model linear layers: \", target_modules)\n",
    "\n",
    "df_grouped_test = df_test.groupby(['variety', 'source', 'task'])\n",
    "for (variety, source, task), test_section in df_grouped_test:\n",
    "  \n",
    "    model_name = f\"{variety}_{source}_{task}\".replace(\" \", \"_\")\n",
    "    finetuned_model_path = os.path.join(finetuned_models_path, model_name)\n",
    "    if not os.path.isdir(finetuned_model_path):\n",
    "        print(f\"Model {model_name} not found (path: {finetuned_model_path})\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, finetuned_model_path, use_safetensors=True)\n",
    "\n",
    "\n",
    "    print(f\"Model: {model_name} <<<<<<<<<<<<<<<\")'''\n",
    "\n",
    "    \n",
    "    ''' check finetuned model lora layers norm '''\n",
    "    '''print(\"2) Check finetuned model lora layers norm\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora\" in name and param.norm()==0:\n",
    "            print(\"Layer with 0 norm:\", name, param.norm())'''\n",
    "\n",
    "    \n",
    "    ''' check merged (finetuned) model and base model difference in parameters'''\n",
    "    '''print(\"3) Check merged (finetuned) model and base model difference in parameters\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        low_cpu_mem_usage=True,\n",
    "        return_dict=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    different = False\n",
    "    for (name_base, p_base), (name_merged, p_merged) in zip(\n",
    "            base_model.named_parameters(), merged_model.named_parameters()):\n",
    "        \n",
    "        if name_base != name_merged:\n",
    "            print(f\"Name mismatch: {name_base} vs {name_merged}\")\n",
    "            continue\n",
    "    \n",
    "        if not torch.equal(p_base, p_merged):\n",
    "            print(f\"Layer {name_base} is different\")\n",
    "            different = True\n",
    "    if not different:\n",
    "        print(\" -- Merged model is identical to base model (bitwise)\")\n",
    "    else:\n",
    "        print(\" -- Merged model differs from base model\")\n",
    "'''\n",
    "    \n",
    "    ''' check if LoRA target modules exist in mistralai/Mistral-7B-Instruct-v0.3 '''\n",
    "    '''print(\"4) Check if LoRA target modules exist in mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "    \n",
    "    lora_config = model.peft_config['default']  # LoraConfig object\n",
    "\n",
    "    print(\"Target modules:\", lora_config.target_modules)\n",
    "    \n",
    "    # Check which target modules exist in the model\n",
    "    for target in lora_config.target_modules:\n",
    "        found = [n for n, _ in base_model.named_modules() if target in n]\n",
    "        if found:\n",
    "            print(f\"Target '{target}' matched layers: {found}\")\n",
    "        else:\n",
    "            print(f\"Target '{target}' not found in base model!\")\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()'''\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9178824,
     "sourceId": 14373066,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9369987,
     "sourceId": 14666870,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
