{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-02T10:08:09.639509Z",
     "iopub.status.busy": "2026-02-02T10:08:09.639205Z",
     "iopub.status.idle": "2026-02-02T13:13:52.920027Z",
     "shell.execute_reply": "2026-02-02T13:13:52.919438Z",
     "shell.execute_reply.started": "2026-02-02T10:08:09.639484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "\n",
    "# config\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 3\n",
    "LR = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "set_seed(2026)\n",
    "\n",
    "# dataset\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.texts = self.df[\"text\"].tolist()\n",
    "        self.labels = self.df[\"label\"].tolist()\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\",\n",
    "            \n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k,v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "\n",
    "# model\n",
    "class SarcasmClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "class SarcasmModel(nn.Module):\n",
    "    def __init__(self, encoder, hidden_size, pos_weight):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = SarcasmClassifier(hidden_size)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        with torch.no_grad():\n",
    "            out = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            h = out.hidden_states[-1]     # [B,T,D]\n",
    "    \n",
    "            # mean pooling\n",
    "            mask = attention_mask.unsqueeze(-1)\n",
    "            h_masked = h * mask\n",
    "            emb = h_masked.sum(dim=1) / mask.sum(dim=1)\n",
    "            emb = emb.float()\n",
    "    \n",
    "        logits = self.classifier(emb)\n",
    "    \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "    \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "# train function\n",
    "def train_model(train_df, test_df, save_path):\n",
    "\n",
    "    print(f\"\\n=== Start Training  ===\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    encoder = AutoModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map={\"\": 0}   # tutto su una GPU\n",
    ")\n",
    "\n",
    "    encoder.eval()\n",
    "    for p in encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    train_ds = SarcasmDataset(train_df, tokenizer)\n",
    "    test_ds  = SarcasmDataset(test_df, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # class imbalance weight\n",
    "    labels = torch.tensor(train_ds.labels)\n",
    "    N_pos = (labels==1).sum().item()\n",
    "    N_neg = (labels==0).sum().item()\n",
    "    pos_weight = torch.tensor([min(N_neg / N_pos, 2.0)]).to(DEVICE)\n",
    "\n",
    "    print(f\"Pos weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "    hidden_size = encoder.config.hidden_size\n",
    "    model = SarcasmModel(encoder, hidden_size, pos_weight).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=LR)\n",
    "\n",
    "    \n",
    "    # train loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | loss = {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    \n",
    "    # eval\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            logits,_ = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).int()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    report_dict = classification_report(all_labels, all_preds, output_dict=True)\n",
    "    \n",
    "    print(\"labels:\", [int(x) for x in all_labels])\n",
    "    print(\"preds: \", [ int(x) for x in all_preds])\n",
    "\n",
    "    print(\"\\nF1-score sarcasm:\", f1)\n",
    "    print(\"\\nClassification report:\\n\", report_dict)\n",
    "\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_df[\"variety\"] = test_df[\"variety\"].iloc[0]\n",
    "    report_df[\"source\"] = test_df[\"source\"].iloc[0]\n",
    "    report_df[\"task\"] = test_df[\"task\"].iloc[0]\n",
    "\n",
    "\n",
    "    # save model\n",
    "    torch.save(model.classifier.state_dict(), save_path)\n",
    "    print(f\"\\nModel saved to {save_path}\")\n",
    "\n",
    "    \n",
    "    # free GPU\n",
    "    del model\n",
    "    del encoder\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return report_df\n",
    "\n",
    "\n",
    "# main\n",
    "if __name__ == \"__main__\":\n",
    "    train = pd.read_csv(\"/kaggle/input/besstie/train.csv\")\n",
    "    test = pd.read_csv(\"/kaggle/input/besstie/valid.csv\")\n",
    "    train = train.dropna(subset=['text', 'label', 'variety', 'source', 'task'])\n",
    "    test = test.dropna(subset=['text', 'label', 'variety', 'source', 'task'])\n",
    "    \n",
    "    train_AU = train[\n",
    "    (train['variety'] == \"en-AU\") &\n",
    "    (train['source'] == \"Reddit\") &\n",
    "    (train['task'] == \"Sarcasm\")\n",
    "    ]\n",
    "\n",
    "    train_IN = train[\n",
    "    (train['variety'] == \"en-IN\") &\n",
    "    (train['source'] == \"Reddit\") &\n",
    "    (train['task'] == \"Sarcasm\")\n",
    "    ]\n",
    "\n",
    "    train_UK = train[\n",
    "    (train['variety'] == \"en-UK\") &\n",
    "    (train['source'] == \"Reddit\") &\n",
    "    (train['task'] == \"Sarcasm\")\n",
    "    ]\n",
    "\n",
    "    test_AU = test[\n",
    "    (test['variety'] == \"en-AU\") &\n",
    "    (test['source'] == \"Reddit\") &\n",
    "    (test['task'] == \"Sarcasm\")\n",
    "    ]\n",
    "\n",
    "    test_IN = test[\n",
    "    (test['variety'] == \"en-IN\") &\n",
    "    (test['source'] == \"Reddit\") &\n",
    "    (test['task'] == \"Sarcasm\")\n",
    "    ]\n",
    "\n",
    "    test_UK = test[\n",
    "    (test['variety'] == \"en-UK\") &\n",
    "    (test['source'] == \"Reddit\") &\n",
    "    (test['task'] == \"Sarcasm\")\n",
    "    ]\n",
    "\n",
    "    all_reports = []\n",
    "    \n",
    "    # UK\n",
    "    all_reports.append(train_model(\n",
    "        train_df=train_UK,\n",
    "        test_df=test_UK,\n",
    "        save_path=\"/kaggle/working/sarcasm_head_uk.pt\"\n",
    "    ))\n",
    "    \n",
    "    # IN\n",
    "    all_reports.append(train_model(\n",
    "        train_df = train_IN,\n",
    "        test_df= test_IN,\n",
    "        save_path=\"/kaggle/working/sarcasm_head_in.pt\"\n",
    "    ))\n",
    "    \n",
    "\n",
    "    # AU\n",
    "    all_reports.append(train_model(\n",
    "        train_df = train_AU,\n",
    "        test_df=test_AU,\n",
    "        save_path=\"/kaggle/working/sarcasm_head_au.pt\"\n",
    "    ))\n",
    "\n",
    "    final_report = pd.concat(all_reports)\n",
    "    final_report.to_csv(\"/kaggle/working/mistral_head_reddit_sarcasm.csv\", index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8969722,
     "sourceId": 14087651,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
